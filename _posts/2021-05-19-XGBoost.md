---
layout:     default
title:      XGBoost
date:       2021-05-19 16:10
author:     喵小六
---

> 阅读本文，你需要基本的高数知识，和了解一些机器学习的术语比如“训练集”、“标签”、“特征”、“学习率”等

在学习 XGBoost 之前，我们先看一个通常的 Gradient Boost 是怎么一步步训练一个模型的，这有助于我们理解 XGBoost

## 一、一个简单的 Gradient Boost 手动实现

我们现在有一个只有 6 个样本的 **训练集**，其中 Weight 是 **标签**，Height, Favorite color, Gender 是 **特征**

![image-20210518201827455](/img/2021-05-19-XGBoost.assets/image-20210518201827455.png)

首先计算所有样本的平均体重，这就是我们的第一个模型了（如下图），用这个模型来预测 6 个样本的体重，预测结果都为 71.2 kg

![image-20210518200912777](/img/2021-05-19-XGBoost.assets/image-20210518200912777.png)

然后，基于第一个模型和真实值的差距构建第二棵树（没错，即使第一个模型只有一个节点，它也是一棵树）

我们称这个差距为 **残差** ，当计算出残差后，便不再关注真实值。

![image-20210518201723277](/img/2021-05-19-XGBoost.assets/image-20210518201723277.png)

先把建树方法放一边，不要在意为什么这么分割特征，假设我们已经通过残差建好了树，如下图（如果你想知道这方面的知识，搜索“决策树”）

![image-20210518202437510](/img/2021-05-19-XGBoost.assets/image-20210518202437510.png)

上图这样分割后，发现有两个叶子节点中存在多个值，用他们的平均值替换一下得到下图

![image-20210518202719795](/img/2021-05-19-XGBoost.assets/image-20210518202719795.png)

上图就是第二棵树了，但这并不是我们的第二个模型，第二个模型还要加上第一棵树，如下图

![image-20210518202830730](/img/2021-05-19-XGBoost.assets/image-20210518202830730.png)

我们来看一下第二个模型的预测值

| Height(m) | Favorite Color | Gender | Weight(kg) | Predicted Weight |
| --------- | -------------- | ------ | ---------- | ---------------- |
| 1.6       | Blue           | Male   | 88         | 71.2 + 16.8 = 88 |
| 1.6       | Green          | Female | 76         | 71.2 + 4.8 = 76  |
| 1.5       | Blue           | Female | 56         | 71.2 - 14.7 = 56.5 |
| 1.8       | Red            | Male   | 73         | 71.2 + 3.8 = 75  |
| 1.5       | Green          | Male   | 77         | 71.2 + 3.8 = 75  |
| 1.4       | Blue           | Female | 57         | 71.2 - 14.7 = 56.5 |

哇，相比第一个模型，第二个模型效果似乎“相当好”

但稍微懂得一点机器学习知识的朋友也能看得出来，这个模型 **过拟合** 了，从第一个模型到第二个模型的学习速度太快了，换句话说，**学习率** 太高了

接下来，我们给第二棵树一个学习率，比如 0.1，让它对整个模型的贡献变小

![image-20210519102307520](/img/2021-05-19-XGBoost.assets/image-20210519102307520.png)

再让我们计算一下改进后的第二个模型的预测值，顺便也求出残差

| Height | Favorite Color | Gender | Weight | Predicted Weight | Residual |
| -- | -- | -- | -- | -- | -- |
| 1.6 | Blue | Male | 88 | 71.2 + 1.68 = 72.88 | 15.12 |
| 1.6 | Green | Female | 76 | 71.2 + 0.48 = 71.68 | 4.32 |
| 1.5 | Blue | Female | 56 | 71.2 - 1.47 = 69.73 | -13.73 |
| 1.8 | Red | Male | 73 | 71.2 + 0.38 = 71.58 | 1.42 |
| 1.5 | Green | Male | 77 | 71.2 + 0.38 = 71.58 | 5.42 |
| 1.4 | Blue | Female | 57 | 71.2 - 1.47 = 69.73 | -12.73 |

这就是第二个模型的效果了

显然，这并不是最终的模型，回顾第二棵树的构建，我们能够构建出第三棵、第四棵……

![image-20210519103750286](/img/2021-05-19-XGBoost.assets/image-20210519103750286.png)

这就是 Gradient Boost 怎么训练的，当然这只是 Gradient Boost 其中一种比较通用的训练方式

接下来进入正题，XGBoost 是怎么优化 Gradient Boost 的

## 二、XGBoost 的原理

### 2.1 定义目标函数

定义 $x_i$ 是第 $i$ 个样本，$f_t(x_i)$ 是第 $t$ 棵树对预测值的贡献，那么当有 $k$ 棵树时，或者说第 $k$ 个模型的预测值为：

$$
\widehat{y}_i=\sum_{t=1}^{k}f_t(x_i)=\sum_{t=1}^{k-1}f_t(x_i)+f_k(x_i)=\widehat{y}_i^{k-1}+f_k(x_i)
$$

从一个通用的目标函数出发

$$
Obj=\sum_{i=1}^{n}l(y_i,\widehat{y}_i)+\Omega(f_k)
$$

**目标函数** 是我们描述模型好坏的指标

其中，$n$ 是样本数量

$l(y_i,\widehat{y}_i)$ 表示预测误差，即预测值 $\widehat{y}_i$ 和真实值 $y_i$ 的差距，术语叫 **损失函数(loss function)**。这个值越大，预测值就越远离真实值，我们的模型在训练集上的效果就越差；这个值越小，预测值就越接近真实值，我们的模型在训练集上的效果就越好

$\Omega(f_k)$ 表示第 $k$ 棵树的模型复杂度，这个值越大，树就越复杂，复杂的树会更切合训练集，也就是过拟合；这个值越小，模型就越简单，模型的泛化性就越高

> 根据 [吴恩达的课程](https://www.bilibili.com/video/BV164411b7dx)，$l(y_i,\widehat{y}_i)$ 是控制 bias 的，$\Omega(f_k)$ 是控制 variance 的，欠拟合的时候 $l(y_i,\widehat{y}_i)$ 和 $\Omega(f_k)$ 都比较大，过拟合的时候，$\Omega(f_k)$ 会比较大

显然，我们的任务就是让目标函数 $Obj$ 尽可能的小

### 2.2 化简目标函数（一）

在化简前明确目标，我们的目的是优化第 $k$ 个模型，而在构建第 $k$ 棵树的时候，前 $k-1$ 棵树其实已经在优化第 $k-1$ 个模型的时候就构建好了（类似数学归纳法），那么在化简中，我们将假设前 $k-1$ 棵树已知

将 (1) 式代入 (2) 式得

$$
Obj=\sum_{i=1}^{n}l(y_i,\widehat{y}_i^{k-1}+f_k(x_i))+\Omega(f_k)
$$

这里引入 [泰勒公式](https://zh.wikipedia.org/wiki/%E6%B3%B0%E5%8B%92%E5%85%AC%E5%BC%8F) 的一元形式的前两阶

$$
f(x)=f(a)+\frac{ {f}'(a)}{1!}(x-a)+\frac{ {f}''(a)}{2!}(x-a)^2
$$

换一种表达形式，即

$$
f(a+\Delta{a})=f(a)+\frac{ {f}'(a)}{1!}\Delta{a}+\frac{ {f}''(a)}{2!}\Delta{a}^2
$$

其中 $f(a)=l(y_i,\widehat{y}_i^{k-1})$ , $f(a+\Delta{a})=l(y_i,\widehat{y}_i^{k-1}+f_k(x_i))$ ，那么 (3) 式用泰勒展开得

$$
Obj=\sum_{i=1}^{n}[l(y_i,\widehat{y}_i^{k-1})+{l(y_i,\widehat{y}_i^{k-1})}'f_k(x_i)+\frac{1}{2}{l(y_i,\widehat{y}_i^{k-1})}''f_k^2(x_i)]+\Omega(f_k)
$$

其中 $l(y_i,\widehat{y}_i^{k-1})$ 在前 $k-1$ 棵树确定的情况下是个已知值，那么它的一阶导数和二阶导数也都是已知值，为方便书写，设已知值 ${l(y_i,\widehat{y}_i^{k-1})}'=g_i\ 和\ {l(y_i,\widehat{y}_i^{k-1})}''=h_i$ ，那么 (6) 式写为

$$
Obj=\sum_{i=1}^{n}[g_if_k(x_i)+\frac{1}{2}h_if_k^2(x_i)]+\Omega(f_k)
$$

到这里，$Obj$ 就只和第 $k$ 棵树 $f_k$ 有关了

### 2.3 树的复杂度

接下来我们假设第 $k$ 棵树的构造已知

定义 $T$ 为叶子节点的个数，如下图，$T=4$

定义 $j$ 为叶子节点的位置，如下图中红色的标记

定义 $w_j$ 为叶子节点 $j$ 的值，如下图，$w_1=-14.7\ ,\ w_2=4.8$，那么

![image-20210519144726824](/img/2021-05-19-XGBoost.assets/image-20210519144726824.png)

定义 $q(x_i)$ 表示样本 $x_i$ 的位置，比如 $q(x_1)=4\ ,\ q(x_2)=2$，那么

$$
f_k(x_i)=w_{q(x_i)}
$$

定义 $I_j$ 为叶子节点 $j$ 包含的样本编号 $i$ ，如下图 $I_1=\{6,3\}\ ,\ I_2=\{2\}$

![image-20210519144640213](/img/2021-05-19-XGBoost.assets/image-20210519144640213.png)

树的复杂度有很多维度可以描述，比如：树的深度、叶子节点的个数、叶子节点权重的平滑程度等等

XGBoost 用两个维度来描述树的复杂度，其一是叶节点个数，其二是叶子节点权重的平滑程度：

$$
\Omega(f_k)=\gamma T+\frac{1}{2}\lambda\sum_{j=1}^{T}w_j^2
$$

其中 $\gamma$ 和 $\lambda$ 是超参数，即我们可以随时改变这两个参数的值，来控制 $T$ 和 $w$ 对 $\Omega(f_k)$ 的影响

### 2.4 化简目标函数（二）

将 (8)、(9) 式代入 (7) 式

$$
\begin{equation}
\begin{aligned}
Obj
&=\sum_{i=1}^{n}(g_i f_k(x_i)+\frac{1}{2}h_i f_k^2(x_i))+\Omega(f_k)\\
&=\sum_{i=1}^{n}(g_i w_{q(x_i)}+\frac{1}{2}h_i w_{q(x_i)}^2)+\gamma T+\frac{1}{2}\lambda\sum_{j=1}^{T}w_j^2\\
&=\sum_{j=1}^{T}((\sum_{i=I_j}g_i)w_j+\frac{1}{2}(\sum_{i=I_j}h_i+\lambda)w_j^2)+\gamma T
\end{aligned}
\end{equation}
$$

未知数 $x_i$ 在下标里，这非常不好

这里用到上一节的定义 $I_j$，以上一节的二图为例来化简 $\sum_{i=1}^{n}{g_iw_{q(x_i)}}$ ：

$$
\begin{equation}
\begin{aligned}
\sum_{i=1}^{n}{g_iw_{q(x_i)}}
&=g_1w_{q(x_1)}+g_2w_{q(x_2)}+g_3w_{q(x_3)}+g_4w_{q(x_4)}+g_5w_{q(x_5)}+g_6w_{q(x_6)}+...+g_nw_{q(x_n)}\\
&=(g_6w_{q(x_6)}+g_3w_{q(x_3)})+(g_2w_{q(x_2)})+(g_4w_{q(x_4)}+g_5w_{q(x_5)})+(g_1w_{q(x_1)})+...\\
&=(g_6+g_3)w_1+g_2w_2+(g_4+g_5)w_3+g_1w_4+...\\
&=\sum_{i\in I_1}g_iw_1+\sum_{i\in I_2}g_iw_2+\sum_{i\in I_3}g_iw_3+\sum_{i\in I_4}g_iw_4+...\\
&=\sum_{j=1}^{T}{(\sum_{i\in I_j}g_i)w_j}
\end{aligned}
\end{equation}
$$

同理

$$
\sum_{i=1}^{n}\frac{1}{2}h_i w_{q(x_i)}^2=\sum_{j=1}^{T}{\frac{1}{2}(\sum_{i\in I_j}h_i)w_j^2}
$$

将 (11)、(12) 式代入 (10) 式得

$$
\begin{equation}
\begin{aligned}
Obj
&=\sum_{i=1}^{n}(g_i w_{q(x_i)}+\frac{1}{2}h_i w_{q(x_i)}^2)+\gamma T+\frac{1}{2}\lambda\sum_{j=1}^{T}w_j^2\\
&=\sum_{j=1}^{T}((\sum_{i=I_j}g_i)w_j+\frac{1}{2}(\sum_{i=I_j}h_i)w_j^2)+\gamma T+\frac{1}{2}\lambda\sum_{j=1}^{T}w_j^2\\
&=\sum_{j=1}^{T}((\sum_{i=I_j}g_i)w_j+\frac{1}{2}(\sum_{i=I_j}h_i+\lambda)w_j^2)+\gamma T
\end{aligned}
\end{equation}
$$

至此，目标函数就变成了一元二次函数求最值

为方便书写，记 $\sum_{i\in I_j}g_i=G_j\ ,\ \sum_{i\in I_j}h_i=H_j$ 得 $w_j$ 的最优解 $-\frac{G_j}{H_j+\lambda}$ ，那么目标函数的最优解：

$$
Obj^*=-\frac{1}{2}\sum_{j=1}^{T}\frac{G_j^2}{H_j+\lambda}+\gamma T
$$

到这里，我们知道了，如果树的构造已知，就可以立即算出目标函数的最优值

接下来，我们将在茫茫多的树中寻找有着最优目标函数值的那个构造

## 三、树的构造

没看完、摸了

## 四、参考文献和视频

- XGBoost 的论文，XGBoost: A Scalable Tree Boosting System - Tianqi Chen [https://arxiv.org/pdf/1603.02754.pdf](https://arxiv.org/pdf/1603.02754.pdf)
- Gradient Boost 的介绍视频，Gradient Boost Part 1 (of 4): Regression Main Ideas [https://www.youtube.com/watch?v=3CC4N4z3GJc](https://www.youtube.com/watch?v=3CC4N4z3GJc)
- xgboost原理？ - 知乎用户的回答 - 知乎 [https://www.zhihu.com/question/58883125/answer/206813653](https://www.zhihu.com/question/58883125/answer/206813653)
- XGBoost的技术剖析 - 贪心学院 [https://www.bilibili.com/video/BV1si4y1G7Jb](https://www.bilibili.com/video/BV1si4y1G7Jb)
